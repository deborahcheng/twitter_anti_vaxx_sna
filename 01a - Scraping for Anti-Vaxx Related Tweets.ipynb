{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraper for Anti-Vaxx Related Tweets from Twitter\n",
    "\n",
    "The objective of this notebook is to scrape tweets under 6 hashtags commonly used by anti-vaxxers on Twitter. The scraped information will be cleaned and analyzed in separated notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime\n",
    "import tweepy\n",
    "import time\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set User Defined Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Twitter API v2 has the ability of searching for entire conversations (including replies, quotes and retweets) by conversation ID. Since this was unavailable on Twitter API v1.1, which all of the Twitter python libraries currently use, we had to define our own functions in order to use Twitter API v2. Twitter API v1.1 will still be used through tweepy for querys unavailable on Twitter API v2 at the time of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Twitter Authorization Codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_to_use = \"deborah\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (tokens_to_use == \"albert\") :\n",
    "    ACCESS_TOKEN = '1277460825599508480-7j6rqWR0NZBCPRZ71pUVyZTKwuaBwf'\n",
    "    ACCESS_SECRET = 'rZ1kGLrc18GJOEBncEA4GCKwUEoQMfBpaVn8qjSXp3rxH'\n",
    "    CONSUMER_KEY = 'Mvmui6zu16TEK6Q1NnJp3mWOg'\n",
    "    CONSUMER_SECRET = 'UsK7AvhtSp0oBEV46Pm4sNzRYc3MAidhauTIX1K5tJKYRllgrc'\n",
    "    #bearer token\n",
    "    token = 'AAAAAAAAAAAAAAAAAAAAAA2AJwEAAAAAgBoQOaUAxzTzGKEE9nOMssJ01o4%3DZVpQV4BIPqPuLFlUsmjnkxOi8kv0IOgpOa8NgytniNFW0K36q3'\n",
    "elif (tokens_to_use == \"deborah\") :\n",
    "    ACCESS_TOKEN = '168323323-6hftUbBs0WYhDNmtn8c7X2sH8PvvWdri1ABKecBI'\n",
    "    ACCESS_SECRET = '4cOcQpqjwKorL0jssomGTUy5nW7d81bVz6TWJp8dwlzEO'\n",
    "    CONSUMER_KEY = 'WcUcc7EOc5Bw4NuHGI5cGgJZ8'\n",
    "    CONSUMER_SECRET = 'xrhvRXPhaJZEbtBo9kshs3pgYYnWabPdNO5rqJy5toT7JPDY5S'\n",
    "    #bearer token\n",
    "    token = 'AAAAAAAAAAAAAAAAAAAAAIFxJwEAAAAAkL0GXCPub0I%2BMW9v3p1CsNBZMrQ%3DWbaCrcrY6miB8TD7E5L5Xrz6ItfTJoxUy3EPDIksC2of4OVySs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup access to API\n",
    "def connect_to_twitter_OAuth():\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "    api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True, retry_count = 10, retry_delay = 10)\n",
    "    return api\n",
    "\n",
    "\n",
    "# Create API object\n",
    "api = connect_to_twitter_OAuth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sleep countdown to avoid maxing out Twitter query window\n",
    "def sleep_countdown(sleep_time) :\n",
    "    while sleep_time >= 0 :\n",
    "        m, s = divmod(sleep_time, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        time_left = str(h).zfill(2) + \":\" + str(m).zfill(2) + \":\" + str(s).zfill(2)\n",
    "        print(time_left + \"\\r\", end = \"\")\n",
    "        time.sleep(1)\n",
    "        sleep_time -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url_query_hashtag (hashtag, max_return) :\n",
    "    \"\"\"URL query for main conversation only, no retweets and no quotes.\"\"\"\n",
    "    query = \" -is:retweet -is:quote lang:en\"\n",
    "    tweet_fields = \"tweet.fields=conversation_id,created_at,in_reply_to_user_id,public_metrics\"\n",
    "    expansions = \"expansions=author_id,referenced_tweets.id\"\n",
    "    max_results = \"max_results=\" + str(max_return)\n",
    "    # start_time = \"start_time=\" + datetime.datetime(year = 2020, month = 11, day = 1).isoformat() + \"Z\"\n",
    "    # end_time = \"end_time=\" + datetime.datetime(year = 2020, month = 11, day = 19).isoformat() + \"Z\"\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent?query={}{}&{}&{}&{}\".format(\n",
    "        hashtag, query, tweet_fields, max_results, expansions#, start_time#, end_time\n",
    "    )\n",
    "    return url\n",
    "\n",
    "def create_url_query_by_tweet_id (tweet_id) :\n",
    "    \"\"\"URL query for specific tweets only, no retweets and no quotes.\"\"\"\n",
    "    tweet_fields = \"tweet.fields=conversation_id\"\n",
    "    expansions = \"expansions=author_id\"\n",
    "#     max_results = \"max_results=\" + str(max_return)\n",
    "#     start_time = \"start_time=\" + datetime.datetime(year = 2020, month = 11, day = 1).isoformat() + \"Z\"\n",
    "#     end_time = \"end_time=\" + datetime.datetime(year = 2020, month = 11, day = 19).isoformat() + \"Z\"\n",
    "    url = \"https://api.twitter.com/2/tweets?ids={}&{}&{}\".format(\n",
    "        tweet_id, tweet_fields, expansions\n",
    "    )\n",
    "    return url\n",
    "\n",
    "def create_url_conversation_id_query (id, max_return, search_type) :\n",
    "    \"\"\"URL query for specific conversations\"\"\"\n",
    "    if (search_type == \"reply\") :\n",
    "        search_type = \" -is:retweet -is:quote\"\n",
    "    if (search_type == \"retweet\") :\n",
    "        search_type = \" is:retweet\"\n",
    "    if (search_type == \"quote\") :\n",
    "        search_type = \" is:quote\"\n",
    "    query = \"conversation_id:\" + str(id) + search_type + \" lang:en\"\n",
    "    tweet_fields = \"tweet.fields=conversation_id,author_id,created_at,in_reply_to_user_id,public_metrics,entities,referenced_tweets\"\n",
    "    expansions = \"expansions=author_id,referenced_tweets.id\"\n",
    "    user_fields = \"user.fields=created_at\"\n",
    "    max_results = \"max_results=\" + str(max_return)\n",
    "    url = \"https://api.twitter.com/2/tweets/search/recent?query={}&{}&{}&{}&{}\".format(\n",
    "        query, tweet_fields, expansions, user_fields, max_results\n",
    "    )\n",
    "    return url\n",
    "\n",
    "def create_user_lookup_url (user_id) :\n",
    "    \"\"\"URL query for specific tweets only, no retweets and no quotes.\"\"\"\n",
    "    user_fields = \"user.fields=created_at,description,location,name,username,pinned_tweet_id,protected,verified,public_metrics\"\n",
    "    url = \"https://api.twitter.com/2/users?ids={}&{}\".format(\n",
    "        user_id, user_fields\n",
    "    )\n",
    "    return url\n",
    "\n",
    "def create_headers (bearer_token) :\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers\n",
    "\n",
    "def connect_to_endpoint (url, headers) :\n",
    "    response = requests.request(\"GET\", url, headers = headers)\n",
    "    if (response.status_code != 200) :\n",
    "        raise Exception(response.status_code, response.text)\n",
    "    return response.json()\n",
    "\n",
    "def main (url) :\n",
    "    bearer_token = token\n",
    "    headers = create_headers(bearer_token)\n",
    "    json_response = connect_to_endpoint(url, headers)\n",
    "    return (json_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_id_to_conversation_id (tweet_id) :\n",
    "    \"\"\"Query to find conversation id by tweet id\"\"\"\n",
    "    if (type(tweet_id) == list) :\n",
    "        tweet_id = str(tweet_id).strip(\"[]\").replace(\" \", \"\").replace(\"'\", \"\")\n",
    "    else :\n",
    "        tweet_id = tweet_id.tolist()\n",
    "        tweet_id = str(tweet_id).strip(\"[]\").replace(\" \", \"\").replace(\"'\", \"\")\n",
    "    url = create_url_query_by_tweet_id(tweet_id)\n",
    "    response = main(url)\n",
    "    response = pd.json_normalize(response[\"data\"])[[\"author_id\", \"conversation_id\", \"id\"]]\n",
    "    return (response)\n",
    "\n",
    "def search_connections_by_conversation_id (id, max_return, search_type): #user to pass through a list of id's\n",
    "    \"\"\"Search replies, quotes and retweets by conversation id\"\"\"\n",
    "    try :\n",
    "        type(id) == list\n",
    "    except :\n",
    "        print(\"Please pass through a list.\")\n",
    "    else :\n",
    "        id = str(id).strip('[]').replace(\" \", \"\")\n",
    "        url = create_url_conversation_id_query(id, max_return, search_type)\n",
    "        return (main(url))\n",
    "\n",
    "def search_by_hashtag (hashtag, max_return):\n",
    "    \"\"\"Query tweets by hashtag\"\"\"\n",
    "    url = create_url_query_hashtag(hashtag, max_return)\n",
    "    return (main(url))\n",
    "\n",
    "def user_lookup (user_id) :\n",
    "    \"\"\"Lookup user information by user id\"\"\"\n",
    "    if (type(user_id) == list) :\n",
    "        user_id = str(user_id).strip(\"[]\").replace(\" \", \"\").replace(\"'\", \"\")\n",
    "    else :\n",
    "        user_id = user_id.tolist()\n",
    "        user_id = str(user_id).strip(\"[]\").replace(\" \", \"\").replace(\"'\", \"\")\n",
    "    url = create_user_lookup_url(user_id)\n",
    "    response = main(url)\n",
    "    response = pd.json_normalize(response[\"data\"])\n",
    "    return (response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replies(conversation_ids) :\n",
    "    \"\"\"Get replies associated specified conversation id\"\"\"\n",
    "    count = 0\n",
    "    sleep_countdown(3)\n",
    "    replies = pd.DataFrame()\n",
    "    r_users = pd.DataFrame()\n",
    "    for i in range(len(conversation_ids)) :\n",
    "        count += 1\n",
    "        sleep_countdown(3)\n",
    "        id = conversation_ids[\"conversation_id\"][i]\n",
    "        result = search_connections_by_conversation_id(id, max_request, \"reply\")\n",
    "        json_keys = result.keys()\n",
    "        if (\"data\" in json_keys) :\n",
    "            d = pd.json_normalize(result[\"data\"])\n",
    "            replies = pd.concat([replies, d], ignore_index = True)\n",
    "            print(\"Query #:\", count, \"Replies retrieved:\", d.shape[0])\n",
    "        if (\"includes\" in json_keys) :\n",
    "            if (\"users\" in list(result[\"includes\"].keys())) :\n",
    "                u = pd.json_normalize(result[\"includes\"][\"users\"])\n",
    "                r_users = pd.concat([r_users, u], ignore_index = True)\n",
    "\n",
    "    if (r_users.shape[0] > 0) :\n",
    "        r_users = r_users[[\"username\", \"created_at\", \"id\"]]\n",
    "        r_users.columns = [\"user_screen_name\", \"user_created_at\", \"author_id\"]\n",
    "        replies_df = replies.join(r_users.set_index(\"author_id\"), on = \"author_id\")\n",
    "    else :\n",
    "        replies_df = pd.DataFrame(columns = ['conversation_id', 'id', 'text', 'in_reply_to_user_id', 'created_at',\n",
    "                                             'author_id', 'referenced_tweets', 'public_metrics.retweet_count',\n",
    "                                             'public_metrics.reply_count', 'public_metrics.like_count',\n",
    "                                             'public_metrics.quote_count', 'entities.mentions',\n",
    "                                             'entities.annotations', 'user_screen_name', 'user_created_at',\n",
    "                                              'user_screen_name', 'user_created_at'])\n",
    "        replies_df = pd.concat([replies_df, replies], ignore_index = True)\n",
    "    replies_df = replies_df.rename(columns = {\"in_reply_to_user_id\" : \"source_user_id\"})\n",
    "    print(\"Total replies retrieved:\", replies_df.shape[0])\n",
    "    return (replies_df[[\"source_user_id\", \"author_id\", \"id\", \"conversation_id\", \"referenced_tweets\", \"created_at\", \"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes(conversation_ids) :\n",
    "    \"\"\"Get quotes associated specified conversation id\"\"\"\n",
    "    count = 0\n",
    "    sleep_countdown(3)\n",
    "    quotes = pd.DataFrame()\n",
    "    q_users = pd.DataFrame()\n",
    "    for i in range(len(conversation_ids)) :\n",
    "        count += 1\n",
    "        sleep_countdown(3)\n",
    "        id = conversation_ids[\"conversation_id\"][i]\n",
    "        result = search_connections_by_conversation_id(id, max_request, \"quote\")\n",
    "        json_keys = result.keys()\n",
    "        if (\"data\" in json_keys) :\n",
    "            d = pd.json_normalize(result[\"data\"])\n",
    "            quotes = pd.concat([quotes, d], ignore_index = True)\n",
    "            print(\"Query #:\", count, \"Quotes retrieved:\", d.shape[0])\n",
    "        if (\"includes\" in json_keys) :\n",
    "            if (\"users\" in list(result[\"includes\"].keys())) :\n",
    "                u = pd.json_normalize(result[\"includes\"][\"users\"])\n",
    "                q_users = pd.concat([q_users, u], ignore_index = True)\n",
    "\n",
    "    if (q_users.shape[0] > 0) :\n",
    "        q_users = q_users[[\"username\", \"created_at\", \"id\"]]\n",
    "        q_users.columns = [\"user_screen_name\", \"user_created_at\", \"author_id\"]\n",
    "        quotes_df = quotes.join(q_users.set_index(\"author_id\"), on = \"author_id\")\n",
    "    else :\n",
    "        quotes_df = pd.DataFrame(columns = ['conversation_id', 'id', 'text', 'in_reply_to_user_id', 'created_at',\n",
    "                                             'author_id', 'referenced_tweets', 'public_metrics.retweet_count',\n",
    "                                             'public_metrics.reply_count', 'public_metrics.like_count',\n",
    "                                             'public_metrics.quote_count', 'entities.mentions',\n",
    "                                             'entities.annotations', 'user_screen_name', 'user_created_at',\n",
    "                                              'user_screen_name', 'user_created_at'])\n",
    "        quotes_df = pd.concat([quotes_df, quotes], ignore_index = True)\n",
    "    quotes_df = quotes_df.rename(columns = {\"in_reply_to_user_id\" : \"source_user_id\"})\n",
    "    print(\"Total quotes retrieved:\", quotes_df.shape[0])\n",
    "    return (quotes_df[[\"source_user_id\", \"author_id\", \"id\", \"conversation_id\", \"referenced_tweets\", \"created_at\", \"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retweets search not working in API v2, so go back to API v1.1 to fetch retweets\n",
    "def get_retweets (conversation_ids) :\n",
    "    \"\"\"Get retweets associated specified conversation id\"\"\"\n",
    "    count = 0\n",
    "    retweets_df = pd.DataFrame()\n",
    "    convo_ids = []\n",
    "    source_user_id = []\n",
    "    for i in range(len(conversation_ids)) :\n",
    "        sleep_countdown(3)\n",
    "        count += 1\n",
    "        id = conversation_ids[\"conversation_id\"][i]\n",
    "        try :\n",
    "            source_user = api.get_status(id).user.id\n",
    "            result = api.retweets(id, max_request)\n",
    "            tweets_list = [[tweet.text, tweet.created_at, tweet.id_str, tweet.user.screen_name, tweet.user.id_str, tweet.user.location, tweet.user.url, tweet.user.verified, tweet.user.followers_count, tweet.user.friends_count, tweet.user.listed_count, tweet.user.created_at, tweet.in_reply_to_status_id_str, tweet.in_reply_to_screen_name, tweet.entities] for tweet in result]\n",
    "            tweets_df = pd.DataFrame(tweets_list)\n",
    "            print(\"Query #:\", count, \"Retweets retrieved:\", tweets_df.shape[0])\n",
    "            retweets_df = pd.concat([retweets_df, tweets_df], ignore_index = True)\n",
    "            l = tweets_df.shape[0]\n",
    "            if (l > 0) :\n",
    "                c = [id] * l\n",
    "                convo_ids.extend(c)\n",
    "                u = [source_user] * l\n",
    "                source_user_id.extend(u)\n",
    "        except :\n",
    "            print(\"Error occurred when fetching conversation id:\", id)\n",
    "            pass\n",
    "    if (retweets_df.shape[0] > 0) :\n",
    "        retweets_df.columns = [\"text\", \"created_at\", \"id\", \"user_screen_name\", \"author_id\", \"user_location\", \"user_url\", \"user_verified\", \"user_followers_count\", \"user_friends_count\", \"user_listed_count\", \"user_created_at\", \"in_reply_to_status_id_str\", \"in_reply_to_screen_name\", \"entities\"]\n",
    "        #twitter API v1.1 does not support conversation ID, manually include in results\n",
    "        retweets_df[\"conversation_id\"] = convo_ids\n",
    "        retweets_df[\"source_user_id\"] = source_user_id\n",
    "    else :\n",
    "        retweets_df = pd.DataFrame(columns = [\"source_user_id\", \"author_id\", \"id\", \"conversation_id\", \"created_at\", \"text\"])\n",
    "    return(retweets_df[[\"source_user_id\", \"author_id\", \"id\", \"conversation_id\", \"created_at\", \"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #fetch referenced tweets\n",
    "# def get_ref_tweets (conversation_ids) :\n",
    "#    \"\"\"Get reference tweets associated specified conversation id\"\"\"\n",
    "#     count = 0\n",
    "#     ref_tweets_df = pd.DataFrame()\n",
    "#     convo_ids = []\n",
    "#     source_user_id = []\n",
    "#     for i in range(len(conversation_ids)) :\n",
    "#         sleep_countdown(3)\n",
    "#         count += 1\n",
    "#         id = conversation_ids[\"id\"][i]\n",
    "# #         print(\"id:\", id)\n",
    "#         source_user = conversation_ids[\"author_id\"][i]\n",
    "#         tweet = api.get_status(id, max_request)\n",
    "#         tweets_list = [tweet.text, tweet.created_at, tweet.id_str, tweet.user.screen_name, tweet.user.id_str, tweet.user.location, tweet.user.url, tweet.user.verified, tweet.user.followers_count, tweet.user.friends_count, tweet.user.listed_count, tweet.user.created_at, tweet.in_reply_to_status_id_str, tweet.in_reply_to_screen_name, tweet.entities]\n",
    "#         tweets_df = pd.DataFrame(tweets_list).transpose()\n",
    "# #         print(\"Query #:\", count, \"Referenced tweets retrieved:\", tweets_df.shape[0])\n",
    "#         ref_tweets_df = pd.concat([ref_tweets_df, tweets_df], ignore_index = True)\n",
    "#         l = tweets_df.shape[0]\n",
    "#         if (l > 0) :\n",
    "#             c = [id] * l\n",
    "#             convo_ids.extend(c)\n",
    "#             u = [source_user] * l\n",
    "#             source_user_id.extend(u)\n",
    "#     if (ref_tweets_df.shape[0] > 0) :\n",
    "#         ref_tweets_df.columns = [\"text\", \"created_at\", \"id\", \"user_screen_name\", \"author_id\", \"user_location\", \"user_url\", \"user_verified\", \"user_followers_count\", \"user_friends_count\", \"user_listed_count\", \"user_created_at\", \"in_reply_to_status_id_str\", \"in_reply_to_screen_name\", \"entities\"]\n",
    "#         #twitter API v1.1 does not support conversation ID, manually include in results\n",
    "#         ref_tweets_df[\"conversation_id\"] = convo_ids\n",
    "#         ref_tweets_df[\"source_user_id\"] = source_user_id\n",
    "#     else :\n",
    "#         ref_tweets_df = pd.DataFrame(columns = [\"source_user_id\", \"author_id\", \"id\", \"conversation_id\", \"created_at\", \"text\"])\n",
    "#     return(ref_tweets_df[[\"source_user_id\", \"author_id\", \"id\", \"conversation_id\", \"created_at\", \"text\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activity (df) :\n",
    "    \"\"\"Separate tweets by action of reply, quote or retweet\"\"\"\n",
    "    result = pd.DataFrame()\n",
    "    for i in df.index :\n",
    "        tweet_id = np.nan\n",
    "        replied_to_id = np.nan\n",
    "        quoted_id = np.nan\n",
    "        row = df.iloc[i]\n",
    "        ref = row[\"referenced_tweets\"]\n",
    "        l = len(ref)\n",
    "        tweet_id = row[\"id\"]\n",
    "        retweeted_id = np.nan\n",
    "        for j in range(l) :\n",
    "            act = ref[j][\"type\"]\n",
    "            if (act == \"replied_to\") :\n",
    "                replied_to_id = ref[j][\"id\"]\n",
    "            elif (act == \"quoted\") :\n",
    "                quoted_id = (ref[j][\"id\"])\n",
    "        l = [tweet_id, replied_to_id, quoted_id, retweeted_id]\n",
    "        result = result.append([l], ignore_index = True)\n",
    "    result.columns = [\"id\", \"replied_to_tweet_id\", \"quoted_tweet_id\", \"retweeted_id\"]\n",
    "    result.set_index(\"id\", inplace = True)\n",
    "    return (result)\n",
    "\n",
    "def get_activity2 (df) :\n",
    "    \"\"\"Separate tweets by action of reply, quote or retweet\"\"\"\n",
    "    result = pd.DataFrame()\n",
    "    for i in df.index :\n",
    "        tweet_id = np.nan\n",
    "        replied_to_id = np.nan\n",
    "        quoted_id = np.nan\n",
    "        row = df.iloc[i]\n",
    "        ref = row[\"referenced_tweets\"]\n",
    "        tweet_id = row[\"id\"]\n",
    "        retweeted_id = np.nan\n",
    "        if (pd.isna(ref)) :\n",
    "            replied_to_id = np.nan\n",
    "            quoted_id = np.nan\n",
    "        else :\n",
    "            l = len(ref)\n",
    "            for j in range(l) :\n",
    "                act = ref[j][\"type\"]\n",
    "                if (act == \"replied_to\") :\n",
    "                    replied_to_id = ref[j][\"id\"]\n",
    "                elif (act == \"quoted\") :\n",
    "                    quoted_id = (ref[j][\"id\"])\n",
    "        act_list = [tweet_id, replied_to_id, quoted_id, retweeted_id]\n",
    "        result = result.append([act_list], ignore_index = True)\n",
    "    result.columns = [\"id\", \"replied_to_tweet_id\", \"quoted_tweet_id\", \"retweeted_id\"]\n",
    "    result.set_index(\"id\", inplace = True)\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_engaged_user_ids(var_list) :\n",
    "    \"\"\"Get user ids\"\"\"\n",
    "    result = pd.DataFrame()\n",
    "    activity = []\n",
    "    for var in var_list :\n",
    "        l = len(globals()[var])\n",
    "        if (l > 0) :\n",
    "            activity.extend([var] * l)\n",
    "            result = pd.concat([result, globals()[var]], ignore_index = True)\n",
    "    result[\"activity_type\"] = activity\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following searches where done everyday from Dec 18th to 24th, 2020 to scrape for anti-vaxx related tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Most Recent Tweets by Common Anti-Vaxx Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define hashtags to pull\n",
    "hashtags_to_query = [\"%23novax\", \"%23antivax\", \"%23CDCwhistleblower\", \"%23vaccineinjury\", \"%23vaxxed\", \"%23cdcfraud\"]\n",
    "\n",
    "#set number of maximum number of results for each request\n",
    "max_request = 100\n",
    "\n",
    "var_list = []\n",
    "for i in range(len(hashtags_to_query)) :\n",
    "    scraped = \"json_tweets_\" + (hashtags_to_query[i][3:]).replace(\" \", \"\")\n",
    "    globals()[scraped] = search_by_hashtag(hashtags_to_query[i], max_request)\n",
    "    var_list.append(scraped)\n",
    "    print(hashtags_to_query[i][3:], \"tweets: \", globals()[scraped][\"meta\"][\"result_count\"])\n",
    "print(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat results into a single dataframe\n",
    "results_data = pd.DataFrame()\n",
    "results_user = pd.DataFrame()\n",
    "\n",
    "for var in var_list :\n",
    "    results_data = pd.concat([results_data, pd.json_normalize(globals()[var][\"data\"])], ignore_index = False)\n",
    "    results_user = pd.concat([results_user, pd.json_normalize(globals()[var][\"includes\"][\"users\"])], ignore_index = False)\n",
    "    \n",
    "results = results_data.join(results_user.set_index(\"id\"), on = \"author_id\").drop_duplicates(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter for top tweets by different users\n",
    "# results_filtered = results.sort_values(\"public_metrics.reply_count\", ascending = False).drop_duplicates(\"author_id\", keep = \"first\")\n",
    "# results_filtered = results[(results[\"public_metrics.retweet_count\"] > 0) & (results[\"public_metrics.reply_count\"] > 0)]\n",
    "results_filtered = results.sort_values(\"public_metrics.reply_count\", ascending = False)\n",
    "results_filtered = results_filtered.reset_index(drop = True)\n",
    "print(\"Total tweets after filtering:\", results_filtered.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get activities and clean up dataframe\n",
    "results_filtered = results_filtered.join(get_activity2(results_filtered), on = \"id\")\n",
    "results_filtered = results_filtered.drop(\"referenced_tweets\", axis = 1)\n",
    "results_filtered = results_filtered.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "results_filtered.to_csv(\"output/2020-12-24 Tweets/initial_results_filtered.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Replies and Retweets for Top Users' Most Recent Tweets\n",
    "\n",
    "### Get Conversation IDs for Recent Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of tweet ids\n",
    "top_recent_tweet_ids = results_filtered[\"id\"].tolist() + results_filtered[\"replied_to_tweet_id\"].tolist() + results_filtered[\"quoted_tweet_id\"].tolist() + results_filtered[\"retweeted_id\"].tolist()\n",
    "top_recent_tweet_ids = pd.Series(top_recent_tweet_ids).drop_duplicates().dropna().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch conversations ids of recent tweets by top user\n",
    "conversation_id_recent_tweets = pd.DataFrame()\n",
    "for i in list(range(len(top_recent_tweet_ids)))[::100] :\n",
    "    start = 1\n",
    "    if (start + 100 < len(top_recent_tweet_ids)) :\n",
    "        end = start + 100\n",
    "    else :\n",
    "        end = len(top_recent_tweet_ids)\n",
    "    results = tweet_id_to_conversation_id(top_recent_tweet_ids[start:end])\n",
    "    conversation_id_recent_tweets = pd.concat([conversation_id_recent_tweets, results], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_id_recent_tweets = conversation_id_recent_tweets.drop_duplicates(\"conversation_id\").reset_index(drop = True)\n",
    "print(\"Conversations to retrieve:\", conversation_id_recent_tweets.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #get tweets and related information on replied to, quoted from or retweeted from tweets\n",
    "# references = get_ref_tweets(conversation_id_recent_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch replies from conversation ids\n",
    "replies = get_replies(conversation_id_recent_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch quotes from conversation ids\n",
    "quotes = get_quotes(conversation_id_recent_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch retweets from conversation ids\n",
    "retweets = get_retweets(conversation_id_recent_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset indices\n",
    "replies = replies.reset_index(drop = True)\n",
    "quotes = quotes.reset_index(drop = True)\n",
    "retweets = retweets.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get activity connections and clean up\n",
    "replies_activity = get_activity(replies)\n",
    "replies = replies.join(replies_activity, on = \"id\")\n",
    "replies = replies.drop(\"referenced_tweets\", axis = 1)\n",
    "replies = replies.drop_duplicates()\n",
    "\n",
    "quotes_activity = get_activity(quotes)\n",
    "quotes = quotes.join(quotes_activity, on = \"id\")\n",
    "quotes = quotes.drop(\"referenced_tweets\", axis = 1)\n",
    "quotes = quotes.drop_duplicates()\n",
    "\n",
    "l = retweets.shape[0]\n",
    "retweets[\"replied_to_tweet_id\"] = [np.nan] * l\n",
    "retweets[\"quoted_tweet_id\"] = [np.nan] * l\n",
    "retweets[\"retweeted_id\"] = retweets[\"conversation_id\"]\n",
    "retweets = retweets.drop_duplicates()\n",
    "\n",
    "# l = references.shape[0]\n",
    "# references[\"replied_to_tweet_id\"] = [np.nan] * l\n",
    "# references[\"quoted_tweet_id\"] = [np.nan] * l\n",
    "# references[\"retweeted_id\"] = references[\"conversation_id\"]\n",
    "# references = references.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export files\n",
    "# references.to_csv(\"output/2020-12-24 Tweets/references.csv\", index = False)\n",
    "replies.to_csv(\"output/2020-12-24 Tweets/replies.csv\", index = False)\n",
    "quotes.to_csv(\"output/2020-12-24 Tweets/quotes.csv\", index = False)\n",
    "retweets.to_csv(\"output/2020-12-24 Tweets/retweets.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat results and export to csv\n",
    "total_results = pd.concat([replies, quotes, retweets], ignore_index = True)\n",
    "total_results.to_csv(\"output/2020-12-24 Tweets/total_results.csv\", float_format = str, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all replied to, quoted and retweeted tweets ids\n",
    "engagement_tweet_ids = total_results[\"replied_to_tweet_id\"].tolist() + total_results[\"quoted_tweet_id\"].tolist() + total_results[\"retweeted_id\"].tolist()\n",
    "\n",
    "#drop NAs and duplicates\n",
    "engagement_tweet_ids = pd.Series(engagement_tweet_ids).dropna().drop_duplicates()\n",
    "\n",
    "#get list of fetched tweets\n",
    "fetched_tweets = np.union1d(total_results[\"id\"], total_results[\"conversation_id\"])\n",
    "\n",
    "#get a list if tweet ids not yet seen in previous fetches\n",
    "more_tweet_ids = engagement_tweet_ids[~engagement_tweet_ids.isin(fetched_tweets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_tweet_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 2nd Degree Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch conversations ids of newly found referenced tweets\n",
    "conversation_id_recent_tweets = pd.DataFrame()\n",
    "for i in list(range(len(more_tweet_ids)))[::100] :\n",
    "    start = 1\n",
    "    if (start + 100 < len(more_tweet_ids)) :\n",
    "        end = start + 100\n",
    "    else :\n",
    "        end = len(more_tweet_ids)\n",
    "    results = tweet_id_to_conversation_id(more_tweet_ids[start:end])\n",
    "    conversation_id_recent_tweets = pd.concat([conversation_id_recent_tweets, results], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_id_recent_tweets = conversation_id_recent_tweets.drop_duplicates(\"conversation_id\").reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation_id_recent_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get tweets and related information on replied to, quoted from or retweeted from tweets\n",
    "# references = get_ref_tweets(conversation_id_recent_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch replies from conversation ids\n",
    "replies = get_replies(conversation_id_recent_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch quotes from conversation ids\n",
    "quotes = get_quotes(conversation_id_recent_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch retweets from conversation ids\n",
    "retweets = get_retweets(conversation_id_recent_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up\n",
    "# references = references.reset_index(drop = True)\n",
    "replies = replies.reset_index(drop = True)\n",
    "quotes = quotes.reset_index(drop = True)\n",
    "retweets = retweets.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get activity connections and clean up\n",
    "replies_activity = get_activity(replies)\n",
    "replies = replies.join(replies_activity, on = \"id\")\n",
    "replies = replies.drop(\"referenced_tweets\", axis = 1)\n",
    "replies = replies.drop_duplicates()\n",
    "\n",
    "quotes_activity = get_activity(quotes)\n",
    "quotes = quotes.join(quotes_activity, on = \"id\")\n",
    "quotes = quotes.drop(\"referenced_tweets\", axis = 1)\n",
    "quotes = quotes.drop_duplicates()\n",
    "\n",
    "l = retweets.shape[0]\n",
    "retweets[\"replied_to_tweet_id\"] = [np.nan] * l\n",
    "retweets[\"quoted_tweet_id\"] = [np.nan] * l\n",
    "retweets[\"retweeted_id\"] = retweets[\"conversation_id\"]\n",
    "retweets = retweets.drop_duplicates()\n",
    "\n",
    "# l = references.shape[0]\n",
    "# references[\"replied_to_tweet_id\"] = [np.nan] * l\n",
    "# references[\"quoted_tweet_id\"] = [np.nan] * l\n",
    "# references[\"retweeted_id\"] = references[\"conversation_id\"]\n",
    "# references = references.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export files\n",
    "# references.to_csv(\"output/2020-12-24 Tweets/extended_references1.csv\", index = False)\n",
    "replies.to_csv(\"output/2020-12-24 Tweets/extended_replies1.csv\", index = False)\n",
    "quotes.to_csv(\"output/2020-12-24 Tweets/extended_quotes1.csv\", index = False)\n",
    "retweets.to_csv(\"output/2020-12-24 Tweets/extended_retweets1.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat extended results and export to csv\n",
    "extended_results1 = pd.concat([replies, quotes, retweets], ignore_index = True) #, references\n",
    "extended_results1.to_csv(\"output/2020-12-24 Tweets/extended_results1.csv\", float_format = str, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_results1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all replied to, quoted and retweeted tweets ids\n",
    "engagement_tweet_ids = extended_results1[\"replied_to_tweet_id\"].tolist() + extended_results1[\"quoted_tweet_id\"].tolist() + extended_results1[\"retweeted_id\"].tolist()\n",
    "\n",
    "#drop NAs and duplicates\n",
    "engagement_tweet_ids = pd.Series(engagement_tweet_ids).dropna().drop_duplicates()\n",
    "\n",
    "#get list of fetched tweets\n",
    "fetched_tweets = np.union1d(np.union1d(fetched_tweets, more_tweet_ids), np.union1d(extended_results1[\"id\"], extended_results1[\"conversation_id\"]))\n",
    "\n",
    "#get a list if tweet ids not yet seen in previous fetches\n",
    "more_tweet_ids = engagement_tweet_ids[~engagement_tweet_ids.isin(fetched_tweets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_tweet_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 3rd Degree Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch conversations ids of newly found referenced tweets\n",
    "conversation_id_recent_tweets = pd.DataFrame()\n",
    "for i in list(range(len(more_tweet_ids)))[::100] :\n",
    "    start = 1\n",
    "    if (start + 100 < len(more_tweet_ids)) :\n",
    "        end = start + 100\n",
    "    else :\n",
    "        end = len(more_tweet_ids)\n",
    "    results = tweet_id_to_conversation_id(more_tweet_ids[start:end])\n",
    "    conversation_id_recent_tweets = pd.concat([conversation_id_recent_tweets, results], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_id_recent_tweets = conversation_id_recent_tweets.drop_duplicates(\"conversation_id\").reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversation_id_recent_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #get tweets and related information on replied to, quoted from or retweeted from tweets\n",
    "# references = get_ref_tweets(conversation_id_recent_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch replies from conversation ids\n",
    "replies = get_replies(conversation_id_recent_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch quotes from conversation ids\n",
    "quotes = get_quotes(conversation_id_recent_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch retweets from conversation ids\n",
    "retweets = get_retweets(conversation_id_recent_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean up\n",
    "# references = references.reset_index(drop = True)\n",
    "replies = replies.reset_index(drop = True)\n",
    "quotes = quotes.reset_index(drop = True)\n",
    "retweets = retweets.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get activity connections and clean up\n",
    "replies_activity = get_activity(replies)\n",
    "replies = replies.join(replies_activity, on = \"id\")\n",
    "replies = replies.drop(\"referenced_tweets\", axis = 1)\n",
    "replies = replies.drop_duplicates()\n",
    "\n",
    "quotes_activity = get_activity(quotes)\n",
    "quotes = quotes.join(quotes_activity, on = \"id\")\n",
    "quotes = quotes.drop(\"referenced_tweets\", axis = 1)\n",
    "quotes = quotes.drop_duplicates()\n",
    "\n",
    "l = retweets.shape[0]\n",
    "retweets[\"replied_to_tweet_id\"] = [np.nan] * l\n",
    "retweets[\"quoted_tweet_id\"] = [np.nan] * l\n",
    "retweets[\"retweeted_id\"] = retweets[\"conversation_id\"]\n",
    "retweets = retweets.drop_duplicates()\n",
    "\n",
    "# l = references.shape[0]\n",
    "# references[\"replied_to_tweet_id\"] = [np.nan] * l\n",
    "# references[\"quoted_tweet_id\"] = [np.nan] * l\n",
    "# references[\"retweeted_id\"] = references[\"conversation_id\"]\n",
    "# references = references.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export files\n",
    "# references.to_csv(\"output/2020-12-24 Tweets/extended_references2.csv\", index = False)\n",
    "replies.to_csv(\"output/2020-12-24 Tweets/extended_replies2.csv\", index = False)\n",
    "quotes.to_csv(\"output/2020-12-24 Tweets/extended_quotes2.csv\", index = False)\n",
    "retweets.to_csv(\"output/2020-12-24 Tweets/extended_retweets2.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#concat extended results and export to csv\n",
    "extended_results2 = pd.concat([replies, quotes, retweets], ignore_index = True) #, references\n",
    "extended_results2.to_csv(\"output/2020-12-24 Tweets/extended_results2.csv\", float_format = str, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended_results2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all replied to, quoted and retweeted tweets ids\n",
    "engagement_tweet_ids = extended_results2[\"replied_to_tweet_id\"].tolist() + extended_results2[\"quoted_tweet_id\"].tolist() + extended_results2[\"retweeted_id\"].tolist()\n",
    "\n",
    "#drop NAs and duplicates\n",
    "engagement_tweet_ids = pd.Series(engagement_tweet_ids).dropna().drop_duplicates()\n",
    "\n",
    "#get list of fetched tweets\n",
    "fetched_tweets = np.union1d(np.union1d(fetched_tweets, more_tweet_ids), np.union1d(extended_results2[\"id\"], extended_results2[\"conversation_id\"]))\n",
    "\n",
    "#get a list if tweet ids not yet seen in previous fetches\n",
    "more_tweet_ids = engagement_tweet_ids[~engagement_tweet_ids.isin(fetched_tweets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_tweet_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our experience, after the 3rd degree fetch, there are no more extended replies, quotes or retweets. If there are, the above process will be repeated until no further extended tweets are gathered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting Connections Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust the initial top user recent tweets in preparation to consolidate the dataframe with all retrieved results\n",
    "results_filtered[\"source_user_id\"] = results_filtered[\"replied_to_tweet_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consolidate all results and clean up\n",
    "final_results = pd.concat([results_filtered[total_results.columns], total_results, extended_results1, extended_results2, extended_results3], ignore_index = True)\n",
    "final_results = final_results.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display issue with replied_to_tweet_id's tweet id being used as a user_id in the source_user_id column\n",
    "#this seemed to be an issue with twitter\n",
    "errors = final_results[final_results[\"replied_to_tweet_id\"] == final_results[\"source_user_id\"]]\n",
    "errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in errors.index :\n",
    "    final_results.loc[i, \"source_user_id\"] = final_results.loc[i, \"author_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export final results to csv\n",
    "final_results.to_csv(\"output/2020-12-24 Tweets/final_results.csv\", float_format = str, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = final_results[[\"source_user_id\", \"author_id\", \"text\"]]\n",
    "edges.columns = [\"source\", \"target\", \"weight\"]\n",
    "edges = edges.dropna()\n",
    "edges = edges.groupby([\"source\", \"target\"]).count().reset_index()\n",
    "edges = edges.sort_values(\"weight\", ascending = False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export edges\n",
    "edges.to_csv(\"output/2020-12-24 Tweets/edges.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve User Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users = final_results[\"author_id\"].tolist() + final_results[\"source_user_id\"].tolist()\n",
    "all_users = pd.Series(all_users).drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = pd.DataFrame()\n",
    "\n",
    "for i in list(range(len(all_users)))[::100] :\n",
    "    sleep_countdown(3)\n",
    "    start = i\n",
    "    if (start + 100 < len(all_users)) :\n",
    "        end = start + 100\n",
    "    else :\n",
    "        end = len(all_users)\n",
    "    try :\n",
    "        results = user_lookup(all_users[start:end])\n",
    "    except :\n",
    "        print(\"Error in batch:\", start, \"to\", end)\n",
    "        pass\n",
    "    nodes = pd.concat([nodes, results], ignore_index = True)\n",
    "\n",
    "nodes = nodes.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export edges\n",
    "nodes.to_csv(\"output/2020-12-24 Tweets/nodes.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Tweet Scraping For All Referenced Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the process of our analysis, we realized that there are some missing tweets. They are either referenced tweets (tweet ids being replied to, quoted or retweeted). The following is an attempt to retrieve them. Unfortunately, some of these tweets and users were already deleted/banned by Twitter by the time we tried to retrieve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User Defined Functions to Retrieve Additional Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url_query_by_tweet_id2 (tweet_id) :\n",
    "    \"\"\"URL query for specific tweets only, no retweets and no quotes.\"\"\"\n",
    "    tweet_fields = \"tweet.fields=conversation_id,author_id,created_at,in_reply_to_user_id,public_metrics,entities,referenced_tweets\"\n",
    "    expansions = \"expansions=author_id,referenced_tweets.id\"\n",
    "    user_fields = \"user.fields=created_at\"\n",
    "#     max_results = \"max_results=\" + str(max_return)\n",
    "#     start_time = \"start_time=\" + datetime.datetime(year = 2020, month = 11, day = 1).isoformat() + \"Z\"\n",
    "#     end_time = \"end_time=\" + datetime.datetime(year = 2020, month = 11, day = 19).isoformat() + \"Z\"\n",
    "    url = \"https://api.twitter.com/2/tweets?ids={}&{}&{}&{}\".format(\n",
    "        tweet_id, tweet_fields, expansions, user_fields\n",
    "    )\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_by_tweet_id (tweet_id) :\n",
    "    \"\"\"Get tweets by tweet id\"\"\"\n",
    "    if (type(tweet_id) == list) :\n",
    "        tweet_id = str(tweet_id).strip(\"[]\").replace(\" \", \"\").replace(\"'\", \"\")\n",
    "    else :\n",
    "        tweet_id = tweet_id.tolist()\n",
    "        tweet_id = str(tweet_id).strip(\"[]\").replace(\" \", \"\").replace(\"'\", \"\")\n",
    "    url = create_url_query_by_tweet_id2(tweet_id)\n",
    "    response = main(url)\n",
    "    response = pd.json_normalize(response[\"data\"])\n",
    "    return (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Combined Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load previously fetched data to determine which tweets are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_tweet = pd.read_csv(\"output/Combined Tweets/2020-12-24 edges_tweet.csv\", dtype = str)\n",
    "nodes_tweet = pd.read_csv(\"output/Combined Tweets/2020-12-24 nodes_tweet.csv\", dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all tweet ids included in the data\n",
    "tweet_ids = edges_tweet[\"Source\"].append(edges_tweet[\"Target\"])\n",
    "tweet_ids = tweet_ids.drop_duplicates()\n",
    "tweet_ids = pd.DataFrame(tweet_ids, columns = [\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of tweet ids without details\n",
    "missing_tweets = tweet_ids[~tweet_ids[\"id\"].isin(nodes_tweet[\"Name\"].tolist())]\n",
    "missing_tweets = missing_tweets[\"id\"].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Missing Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_results = 100\n",
    "\n",
    "missing_tweets_results = pd.DataFrame()\n",
    "for i in list(range(len(missing_tweets)))[::100] :\n",
    "    start = 1\n",
    "    if (start + 100 < len(missing_tweets)) :\n",
    "        end = start + 100\n",
    "    else :\n",
    "        end = len(missing_tweets)\n",
    "    results = search_by_tweet_id(missing_tweets[start:end])\n",
    "    missing_tweets_results = pd.concat([missing_tweets_results, results], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns to match existing data\n",
    "missing_tweets_results = missing_tweets_results.rename(columns = {\"in_reply_to_user_id\" : \"source_user_id\"})\n",
    "missing_tweets_results = missing_tweets_results[[\"source_user_id\", \"author_id\", \"id\", \"conversation_id\", \"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_user_id</th>\n",
       "      <th>author_id</th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>19098792</td>\n",
       "      <td>1338829441112428544</td>\n",
       "      <td>1338829441112428544</td>\n",
       "      <td>It's a lot better than getting sick : Trial ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1041258824147976192</td>\n",
       "      <td>1339553106284212224</td>\n",
       "      <td>1339553106284212224</td>\n",
       "      <td>I think that its hilarious that people believe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1271769441748074497</td>\n",
       "      <td>1269662880762343424</td>\n",
       "      <td>1339655590138798081</td>\n",
       "      <td>1339646731559067656</td>\n",
       "      <td>@sheepskindee @AndyInLondon1 @Nigel_Farage Per...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>42836999</td>\n",
       "      <td>1338838777616363520</td>\n",
       "      <td>1338838777616363520</td>\n",
       "      <td>There are more than 10,000 active COVID-19 cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1249607420508286977</td>\n",
       "      <td>1338881028908326914</td>\n",
       "      <td>1338881028908326914</td>\n",
       "      <td>#Canada creates a Vaccine Injury Compensation ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        source_user_id            author_id                   id  \\\n",
       "0                  NaN             19098792  1338829441112428544   \n",
       "1                  NaN  1041258824147976192  1339553106284212224   \n",
       "2  1271769441748074497  1269662880762343424  1339655590138798081   \n",
       "3                  NaN             42836999  1338838777616363520   \n",
       "4                  NaN  1249607420508286977  1338881028908326914   \n",
       "\n",
       "       conversation_id                                               text  \n",
       "0  1338829441112428544  It's a lot better than getting sick : Trial ...  \n",
       "1  1339553106284212224  I think that its hilarious that people believe...  \n",
       "2  1339646731559067656  @sheepskindee @AndyInLondon1 @Nigel_Farage Per...  \n",
       "3  1338838777616363520  There are more than 10,000 active COVID-19 cas...  \n",
       "4  1338881028908326914  #Canada creates a Vaccine Injury Compensation ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_tweets_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving User IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of author ids to fetch user informations\n",
    "all_users = missing_tweets_results[\"author_id\"]\n",
    "all_users = pd.Series(all_users).drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:00:00\r"
     ]
    }
   ],
   "source": [
    "# get a user information from missing tweets list\n",
    "nodes = pd.DataFrame()\n",
    "\n",
    "for i in list(range(len(all_users)))[::100] :\n",
    "    sleep_countdown(3)\n",
    "    start = i\n",
    "    if (start + 100 < len(all_users)) :\n",
    "        end = start + 100\n",
    "    else :\n",
    "        end = len(all_users)\n",
    "    try :\n",
    "        results = user_lookup(all_users[start:end])\n",
    "    except :\n",
    "        print(\"Error in batch:\", start, \"to\", end)\n",
    "        pass\n",
    "    nodes = pd.concat([nodes, results], ignore_index = True)\n",
    "\n",
    "nodes = nodes.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 13)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export missing tweet results\n",
    "missing_tweets_results.to_csv(\"output/2021-01-09 Missing Tweets/missing_tweet_results.csv\", index = False)\n",
    "nodes.to_csv(\"output/2021-01-09 Missing Tweets/missing_nodes.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
