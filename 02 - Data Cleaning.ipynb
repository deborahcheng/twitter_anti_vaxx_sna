{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "This notebook combines the previously scraped twitter data, cleans and add any additional columns needed prior to analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kooritsuki\\Anaconda3\\lib\\site-packages\\statsmodels\\tools\\_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import networkx as nx\n",
    "import community as louvain\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Combine Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load all tweets and user information scraped from Dec 18 - 24th, 2020 and combine them into a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all tweets scraped\n",
    "df_connections1 = pd.read_csv(\"output/2020-12-18 Tweets/final_results.csv\", dtype = str)\n",
    "df_connections2 = pd.read_csv(\"output/2020-12-19 Tweets/final_results.csv\", dtype = str)\n",
    "df_connections3 = pd.read_csv(\"output/2020-12-20 Tweets/final_results.csv\", dtype = str)\n",
    "df_connections4 = pd.read_csv(\"output/2020-12-21 Tweets/final_results.csv\", dtype = str)\n",
    "df_connections5 = pd.read_csv(\"output/2020-12-22 Tweets/final_results.csv\", dtype = str)\n",
    "df_connections6 = pd.read_csv(\"output/2020-12-23 Tweets/final_results.csv\", dtype = str)\n",
    "df_connections7 = pd.read_csv(\"output/2020-12-24 Tweets/final_results.csv\", dtype = str)\n",
    "df_connections8 = pd.read_csv(\"output/2021-01-09 Missing Tweets/missing_tweet_results.csv\", dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined all tweets scraped into a single dataframe\n",
    "df_connections = pd.concat([df_connections1, df_connections2, df_connections3, df_connections4,\n",
    "                            df_connections5, df_connections6, df_connections7, df_connections8],\n",
    "                           ignore_index = True)\n",
    "\n",
    "# drop all duplicated entries\n",
    "df_connections = df_connections.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows in df_connections1: 2286\n",
      "# of rows in df_connections2: 2527\n",
      "# of rows in df_connections3: 2385\n",
      "# of rows in df_connections4: 2981\n",
      "# of rows in df_connections5: 3746\n",
      "# of rows in df_connections6: 3797\n",
      "# of rows in df_connections7: 4929\n",
      "# of rows in df_connections8: 1080\n",
      "# of rows after consolidation and dropping duplicates: 8946\n"
     ]
    }
   ],
   "source": [
    "# check for number of rows\n",
    "print(\"# of rows in df_connections1:\", df_connections1.shape[0])\n",
    "print(\"# of rows in df_connections2:\", df_connections2.shape[0])\n",
    "print(\"# of rows in df_connections3:\", df_connections3.shape[0])\n",
    "print(\"# of rows in df_connections4:\", df_connections4.shape[0])\n",
    "print(\"# of rows in df_connections5:\", df_connections5.shape[0])\n",
    "print(\"# of rows in df_connections6:\", df_connections6.shape[0])\n",
    "print(\"# of rows in df_connections7:\", df_connections7.shape[0])\n",
    "print(\"# of rows in df_connections8:\", df_connections8.shape[0])\n",
    "print(\"# of rows after consolidation and dropping duplicates:\", df_connections.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the twitter users who was banned during our scraping process was lotusoak2. We were able to retrieve his information due to a previous scrape. Prior to banning, this user was a very active anti-vax bot, posting dozens of tweets and retweets everyday. Due to the account ban and the deletion of his tweets, most of this accounts post are no longer included in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get lotusoak2's user information from an old retrievement before the account was suspended\n",
    "user_lotusoak = pd.read_csv(\"output/followers_ids_1000/followers2 - OLD.csv\", dtype = str)\n",
    "user_lotusoak = user_lotusoak[user_lotusoak[\"id\"] == \"424664120\"]\n",
    "user_lotusoak = user_lotusoak.drop_duplicates(\"id\")\n",
    "user_lotusoak[\"source_id\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all user information scraped\n",
    "df_users1 = pd.read_csv(\"output/2020-12-18 Tweets/nodes.csv\", dtype = str)\n",
    "df_users2 = pd.read_csv(\"output/2020-12-19 Tweets/nodes.csv\", dtype = str)\n",
    "df_users3 = pd.read_csv(\"output/2020-12-20 Tweets/nodes.csv\", dtype = str)\n",
    "df_users4 = pd.read_csv(\"output/2020-12-21 Tweets/nodes.csv\", dtype = str)\n",
    "df_users5 = pd.read_csv(\"output/2020-12-22 Tweets/nodes.csv\", dtype = str)\n",
    "df_users6 = pd.read_csv(\"output/2020-12-23 Tweets/nodes.csv\", dtype = str)\n",
    "df_users7 = pd.read_csv(\"output/2020-12-24 Tweets/nodes.csv\", dtype = str)\n",
    "df_users8 = pd.read_csv(\"output/2021-01-09 Missing Tweets/missing_nodes.csv\", dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined all user information scraped into a single dataframe\n",
    "df_users = pd.concat([df_users1, df_users2, df_users3, df_users4, df_users5, df_users6, df_users7,\n",
    "                      df_users8, user_lotusoak], ignore_index = True)\n",
    "\n",
    "#drop duplicated users\n",
    "df_users = df_users.drop_duplicates(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of rows in df_users1: 1949\n",
      "# of rows in df_users2: 2028\n",
      "# of rows in df_users3: 1928\n",
      "# of rows in df_users4: 2552\n",
      "# of rows in df_users5: 3091\n",
      "# of rows in df_users6: 3129\n",
      "# of rows in df_users7: 4337\n",
      "# of rows in df_users8: 70\n",
      "# of rows in user_lotusoak: 1\n",
      "# of rows after consolidation and dropping duplicates: 7170\n"
     ]
    }
   ],
   "source": [
    "# check number of rows\n",
    "print(\"# of rows in df_users1:\", df_users1.shape[0])\n",
    "print(\"# of rows in df_users2:\", df_users2.shape[0])\n",
    "print(\"# of rows in df_users3:\", df_users3.shape[0])\n",
    "print(\"# of rows in df_users4:\", df_users4.shape[0])\n",
    "print(\"# of rows in df_users5:\", df_users5.shape[0])\n",
    "print(\"# of rows in df_users6:\", df_users6.shape[0])\n",
    "print(\"# of rows in df_users7:\", df_users7.shape[0])\n",
    "print(\"# of rows in df_users8:\", df_users8.shape[0])\n",
    "print(\"# of rows in user_lotusoak:\", user_lotusoak.shape[0])\n",
    "print(\"# of rows after consolidation and dropping duplicates:\", df_users.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Exisitance of Original Anti-Vax Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add columns to tweets dataframe to identify anti-vaxx hashtags\n",
    "\n",
    "novax = []\n",
    "antivax = []\n",
    "cdcwhistleblower = []\n",
    "vaccineinjury = []\n",
    "vaxxed = []\n",
    "cdcfraud = []\n",
    "\n",
    "for i in df_connections.index :\n",
    "    text = df_connections[\"text\"][i]\n",
    "    if \"#novax\" in text :\n",
    "        novax.append(1)\n",
    "    else :\n",
    "        novax.append(0)\n",
    "    if \"#antivax\" in text :\n",
    "        antivax.append(1)\n",
    "    else :\n",
    "        antivax.append(0)\n",
    "    if \"#cdcwhistleblower\" in text :\n",
    "        cdcwhistleblower.append(1)\n",
    "    else :\n",
    "        cdcwhistleblower.append(0)\n",
    "    if \"#vaccineinjury\" in text :\n",
    "        vaccineinjury.append(1)\n",
    "    else :\n",
    "        vaccineinjury.append(0)\n",
    "    if \"#vaxxed\" in text :\n",
    "        vaxxed.append(1)\n",
    "    else :\n",
    "        vaxxed.append(0)\n",
    "    if \"#cdcfraud\" in text :\n",
    "        cdcfraud.append(1)\n",
    "    else :\n",
    "        cdcfraud.append(0)\n",
    "\n",
    "df_connections[\"hashtag_novax\"] = novax\n",
    "df_connections[\"hashtag_antivax\"] = antivax\n",
    "df_connections[\"hashtag_cdcwhistleblower\"] = cdcwhistleblower\n",
    "df_connections[\"hashtag_vaccineinjury\"] = vaccineinjury\n",
    "df_connections[\"hashtag_vaxxed\"] = vaxxed\n",
    "df_connections[\"hashtag_cdcfraud\"] = cdcfraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delete Non-Vaccine Related Conversations and Separately Classify Pro-Vaxx Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#non-vaxx related conversations\n",
    "non_vax = [\"1339388053312270337\", \"1338969616870043649\", \"1339030239104684034\", \"1338952212903469058\",\n",
    "            \"1338200367520051200\", \"1339380932453703681\", \"1339365858678730752\", \"1339482593608265728\",\n",
    "            \"1339918799336845316\", \"1339996165773946880\", \"1339379035281915904\", \"1339953954994745345\",\n",
    "            \"1339118105570627584\", \"1339409746202251265\", \"1339414394434854912\", \"1339413581016694784\",\n",
    "            \"1339415314052784130\", \"1337800649052971008\", \"1337765454044155905\", \"1339789526328365056\",\n",
    "            \"1339758947624873984\", \"1339884959138533376\", \"1339953895129407489\", \"1340151293470359552\",\n",
    "            \"1340130416292872192\", \"1340375819219316739\", \"1340367744596922368\", \"1340210236641202176\",\n",
    "            \"1340138727633403906\", \"1340588046413164544\", \"1340549469373407233\", \"1340520673349873665\",\n",
    "            \"1340443581064986624\", \"1340806702069915650\", \"1340706934165561346\", \"1340720711451721737\",\n",
    "            \"1341646772054200320\", \"1341125324495609856\", \"1341157386715852801\", \"1341751074626134019\",\n",
    "            \"1341737824182181888\", \"1342032447727411201\", \"1342033228291567617\", \"1342045857244454912\",\n",
    "            \"1342074921225224192\", \"1342077527242788864\"]\n",
    "\n",
    "#pro-vax conversation starter\n",
    "pro_vax = [\"1339977177215864832\", \"1339964294205644801\", \"1339941480715202562\", \"1340089843431432194\",\n",
    "           \"1340043296467537924\", \"1339877425916895237\", \"1339966479870128128\", \"1339912370135699456\",\n",
    "           \"1339746759728349188\", \"1339030239104684034\", \"1340079856881549315\", \"1340073476544110592\",\n",
    "           \"1340029547102801920\", \"1339964673190346752\", \"1339719347057512449\", \"1339810557159899136\",\n",
    "           \"1339737699239014400\", \"1339104569134108673\", \"1339337119102345216\", \"1339373768867721217\",\n",
    "           \"1340096675336085509\", \"1340055177965969408\", \"1339795110117126144\", \"1339858348636684289\",\n",
    "           \"1339869889276878848\", \"1339871792505249793\", \"1339884210312626176\", \"1339883453886701568\",\n",
    "           \"1340003033514708998\", \"1339988673589338113\", \"1340012317178691584\", \"1340024326725263362\",\n",
    "           \"1340039535619674114\", \"1339994129477058560\", \"1339926719239761924\", \"1339936903852937217\", \n",
    "           \"1339943707706048513\", \"1339946410335072257\", \"1339948903798550528\", \"1339948908760469506\",\n",
    "           \"1339963994442969092\", \"1340409103043182592\", \"1340391271777505280\", \"1340358203377393667\",\n",
    "           \"1340141733808656384\", \"1340163822103904256\", \"1340369588089925632\", \"1340371706679091201\",\n",
    "           \"1340371714560192512\", \"1340401519993053186\", \"1340401898877280258\", \"1340359157787062272\",\n",
    "           \"1340280089855057920\", \"1340316181136355328\", \"1340320030043586562\", \"1340326413887483904\",\n",
    "           \"1340328526432657409\", \"1340329716553805826\", \"1340146728285184000\", \"1340175552888639489\",\n",
    "           \"1340417017044713472\", \"1340690524072775680\", \"1340694484477116417\", \"1340628813710016512\",\n",
    "           \"1340590137852497920\", \"1340591279323942914\", \"1340593741078306817\", \"1340609468514615299\",\n",
    "           \"1340621485409357824\", \"1340638855297458176\", \"1340647546822209548\", \"1340658576889659392\", \n",
    "           \"1340669956913029120\", \"1340673684491677703\", \"1340688411791863808\", \"1340577939096780800\", \n",
    "           \"1340563660649861121\", \"1340447163806076929\", \"1340507672014188546\", \"1340535566736220161\", \n",
    "           \"1340538249899634688\", \"1340552918127292418\", \"1340440166339633153\", \"1340703936735490048\",\n",
    "           \"1340796796298293256\", \"1340701332676562944\", \"1340703925318656004\", \"1340718687171522560\",\n",
    "           \"1340719018932654080\", \"1340747429977710593\", \"1340749221595635724\", \"1340824591686615044\",\n",
    "           \"1340900385838526464\", \"1340900393266634753\", \"1340991028363489280\", \"1340794462121947138\",\n",
    "           \"1340764307034943488\", \"1340764316321112064\", \"1340779419774947333\", \"1341005946668105728\", \n",
    "           \"1340968352781516800\", \"1341334935471271938\", \"1341379857482170370\", \"1341419840116965377\", \n",
    "           \"1341465325099954182\", \"1341049209454247936\", \"1341442116874948609\", \"1341177788825518085\", \n",
    "           \"1341571312419004416\", \"1341204484312068096\", \"1341534894460084226\", \"1341579710904582157\", \n",
    "           \"1341582658657275906\", \"1341594850525933568\", \"1341635905665187841\", \"1341640071523557376\", \n",
    "           \"1341655136838496266\", \"1341655151887704065\", \"1341655163954716673\", \"1341660449062998016\", \n",
    "           \"1341511710784499712\", \"1341572415999119361\", \"1341519257570603010\", \"1341036136228122629\",\n",
    "           \"1341053946337456128\", \"1341634930812567552\", \"1341036141009653763\", \"1341055582623698944\", \n",
    "           \"1341066274386948097\", \"1341428687510663169\", \"1341295720956112896\", \"1341307899113517056\",\n",
    "           \"1341383326985310210\", \"1341413569594798087\", \"1341428700856913921\", \"1341292823400865796\",\n",
    "           \"1341443748761382914\", \"1341485645395611650\", \"1341247587815469056\", \"1341073940526080003\", \n",
    "           \"1341073946347761665\", \"1341081415715307522\", \"1341103434506924033\", \"1341126712940236801\",\n",
    "           \"1341133416096477185\", \"1341156874616512512\", \"1341806497798471683\", \"1341778869674672128\",\n",
    "           \"1341810094586163200\", \"1341791010922393600\", \"1341745903191941120\", \"1341791006220570624\",\n",
    "           \"1341798349431779331\", \"1341806064304607232\", \"1341806073964052482\", \"1341815754367381508\", \n",
    "           \"1341716018742382592\", \"1341884107798806528\", \"1341899794835529728\", \"1342213747566374918\", \n",
    "           \"1342213739714662409\", \"1341881623697117185\", \"1341896757026779136\", \"1341972354902142976\",\n",
    "           \"1342002518180687878\", \"1342017554936705025\", \"1341881612989038594\", \"1342100410052583424\", \n",
    "           \"1341821258082693125\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete non-vaxx conversations\n",
    "df_connections = df_connections[~df_connections[\"conversation_id\"].isin(non_vax)]\n",
    "\n",
    "#classify tweets between pro-vaxx and anti-vaxx\n",
    "df_connections[\"stance\"] = df_connections[\"conversation_id\"]\n",
    "df_connections.loc[df_connections[\"stance\"].isin(pro_vax), \"stance\"] = \"Pro-Vaxx\" #pro-vaxx\n",
    "df_connections.loc[(df_connections[\"stance\"] != \"Pro-Vaxx\"), \"stance\"] = \"Anti-Vaxx\" #anti-vax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = df_connections[df_connections[\"id\"].isin(pro-vaxx)]\n",
    "\n",
    "# for i in y.index :\n",
    "#     print(\"Tweet:\", df_connections[\"id\"].loc[i])\n",
    "#     print(df_connections[\"text\"].loc[i])\n",
    "#     print(\"--------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace User IDs with Usernames in Tweets Dataframe\n",
    "\n",
    "After repeatingly having trouble with using Gelphi, we decided to switch to a different application for SNA graphing - Polinode. Whil Polinode does not hand networks with more than 50,000 nodes (Gelphi handles up to 100,000), it is sufficient for our project.\n",
    "\n",
    "Polinode requires uploading edges and nodes in excel format. However, long IDs are cut off read as scientific notations in excel. Therefore, we must use usernames directly in our edges table instead of relying on the IDs to sync from the both the edges and nodes table in excel to sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract usernames and user id\n",
    "usernames = df_users[[\"id\", \"username\"]].set_index(\"id\")\n",
    "usernames.columns = [\"author_username\"]\n",
    "\n",
    "#join usernames with author_id on tweet dataframe\n",
    "df_connections = df_connections.join(usernames, on = \"author_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change column name before joining for source usernames\n",
    "usernames.columns = [\"source_username\"]\n",
    "\n",
    "#join usernames with source_user_id on tweet dataframe\n",
    "df_connections = df_connections.join(usernames, on = \"source_user_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Analysis\n",
    "\n",
    "Use NLP and K-Means to cluster the different topics discussed in the tweets dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !python -m spacy download en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import jaccard_score\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from string import punctuation\n",
    "import collections\n",
    "from collections import Counter\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1339977177215864832</th>\n",
       "      <td>Remember when #antivax charlatan @delbigtree a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338876187976863744</th>\n",
       "      <td>Yale #STUDY Finds Link Between #Vaccination an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339294501375004675</th>\n",
       "      <td>This film stops the lie, which producer Polly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339448531074093056</th>\n",
       "      <td>#Glyphosate Found in Childhood #Vaccines\\n\\nht...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340048362146570242</th>\n",
       "      <td>#China plans to let victims sue #vaccine manuf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text\n",
       "id                                                                    \n",
       "1339977177215864832  Remember when #antivax charlatan @delbigtree a...\n",
       "1338876187976863744  Yale #STUDY Finds Link Between #Vaccination an...\n",
       "1339294501375004675  This film stops the lie, which producer Polly ...\n",
       "1339448531074093056  #Glyphosate Found in Childhood #Vaccines\\n\\nht...\n",
       "1340048362146570242  #China plans to let victims sue #vaccine manuf..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# isolate only the text and the tweet ids to be analyzed\n",
    "tweets = df_connections[[\"id\", \"text\"]].set_index(\"id\")\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14095"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#define punctuations and stopwords\n",
    "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']', '{', '}',\"%\"]\n",
    "stop_words = set(stopwords.words('english')).union(punc)\n",
    "\n",
    "#vectorize individual tweets\n",
    "text = tweets[\"text\"].values\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words)\n",
    "X = vectorizer.fit_transform(text)\n",
    "word_features = vectorizer.get_feature_names()\n",
    "len(word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# apply stemmer and tokenizer to tweets\n",
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "\n",
    "def tokenize(text):\n",
    "    return [stemmer.stem(word) for word in tokenizer.tokenize(text.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kooritsuki\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'ani', 'becaus', 'befor', 'doe', 'dure', 'onc', 'onli', 'ourselv', \"should'v\", 'themselv', 'veri', 'whi', \"you'r\", \"you'v\", 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11602"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test out vectorizers\n",
    "# vectorizer2 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize)\n",
    "# X2 = vectorizer2.fit_transform(text)\n",
    "# word_features2 = vectorizer2.get_feature_names()\n",
    "# len(word_features2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorize while restricting maximum features to 1,000\n",
    "vectorizer3 = TfidfVectorizer(stop_words = stop_words, tokenizer = tokenize, max_features = 1000)\n",
    "X3 = vectorizer3.fit_transform(text)\n",
    "word_features3 = vectorizer3.get_feature_names()\n",
    "len(word_features3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kooritsuki\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:939: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  \" removed in 0.25.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 2\n",
      "0 : rt, vaccin, peopl, chriswicknew, one, get, whi, georebekah, take, like\n",
      "1 : co, https, vaccin, rt, covid, realdonaldtrump, get, receiv, drmadej, nurs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kooritsuki\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:939: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  \" removed in 0.25.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 3\n",
      "0 : vaccin, realdonaldtrump, peopl, get, chriswicknew, like, take, covid, want, think\n",
      "1 : rt, vaccin, peopl, laptop, georgia, dear, voter, florida, governor, bring\n",
      "2 : co, https, vaccin, rt, covid, get, drmadej, watch, nurs, receiv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kooritsuki\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:939: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  \" removed in 0.25.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 4\n",
      "0 : vaccin, realdonaldtrump, peopl, chriswicknew, get, like, take, co, https, covid\n",
      "1 : laptop, georgia, dear, voter, florida, governor, bring, imagin, girl, georebekah\n",
      "2 : co, https, rt, vaccin, covid, drmadej, nurs, watch, get, receiv\n",
      "3 : rt, vaccin, peopl, refus, opinion, evil, moderna, realdonaldtrump, get, pfizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kooritsuki\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:939: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  \" removed in 0.25.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 5\n",
      "0 : vaccin, realdonaldtrump, peopl, get, chriswicknew, like, covid, take, want, think\n",
      "1 : laptop, georgia, dear, voter, florida, governor, bring, imagin, girl, georebekah\n",
      "2 : co, https, rt, vaccin, covid, drmadej, receiv, dircdcyrqc, antivax, realdonaldtrump\n",
      "3 : rt, peopl, vaccin, opinion, evil, moderna, pfizer, deni, whi, correct\n",
      "4 : vaccin, get, rt, nurs, co, https, covid, pl, secret, meet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kooritsuki\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:939: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  \" removed in 0.25.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 10\n",
      "0 : https, co, vaccin, realdonaldtrump, covid, antivax, chriswicknew, peopl, get, amp\n",
      "1 : realdonaldtrump, overwhelm, immedi, approv, start, vaccin, distribut, moderna, rollout, review\n",
      "2 : vaccin, covid, co, https, rt, get, nurs, watch, hous, secret\n",
      "3 : rt, peopl, vaccin, refus, opinion, bar, wear, whi, deni, show\n",
      "4 : laptop, georgia, dear, voter, florida, governor, bring, imagin, girl, georebekah\n",
      "5 : drmadej, dircdcyrqc, co, https, rt, gskfjgebju, nigel, farag, happen, hsrywrrrl\n",
      "6 : evil, probabl, correct, gtconway, rt, anaphylaxi, rfk, jr, fightback, counsel\n",
      "7 : sbp, qokm, unbeliev, equal, racial, true, distribut, marklevinshow, even, co\n",
      "8 : pollytommey, viral, interview, produc, event, current, chanc, busydrt, listen, yet\n",
      "9 : safe, gwinezrx, vasovag, teamtrump, worri, absolut, littl, vice, gsx, bdzkdr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kooritsuki\\Anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:939: FutureWarning: 'n_jobs' was deprecated in version 0.23 and will be removed in 0.25.\n",
      "  \" removed in 0.25.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clusters: 15\n",
      "0 : vaccin, realdonaldtrump, peopl, get, covid, chriswicknew, like, take, richardursomd, know\n",
      "1 : co, covid, https, pelosi, nanci, speaker, pgswmksmfj, wfpfupxp, vaccin, rt\n",
      "2 : peopl, deni, wear, refus, mask, opinion, chriswicknew, rt, vaccin, bu\n",
      "3 : probabl, correct, gtconway, rfk, jr, fightback, counsel, wise, josephjflynn, rt\n",
      "4 : anti, vaxxer, pl, secret, meet, today, plan, expos, ccdhate, stop\n",
      "5 : penc, mike, teamtrump, receiv, watch, coronavirus, vice, bdzkdr, gsx, presid\n",
      "6 : distribut, overwhelm, immedi, approv, sbp, qokm, unbeliev, racial, equal, start\n",
      "7 : co, https, drmadej, rt, dircdcyrqc, vaxx, realdonaldtrump, antivax, vaccin, covid\n",
      "8 : preciouslindi, gatesfound, nihdirector, tx, gat, nih, connect, billgat, theori, conspiraci\n",
      "9 : howleyreport, herebi, volunt, video, refus, live, take, abroad, multi, run\n",
      "10 : evil, wrought, industri, pure, pharmaceut, corrupt, realcandaceo, fauci, dr, gate\n",
      "11 : laptop, georgia, dear, voter, florida, governor, bring, imagin, girl, georebekah\n",
      "12 : phone, easili, download, code, someon, bar, way, andrewyang, show, like\n",
      "13 : wg, edwvf, carlson, rupert, murdoch, tucker, skeptic, push, j, thedailybeast\n",
      "14 : vaccin, rt, nurs, get, whi, ebxfl, uv, short, faint, kimonaq\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# use K-means to separate dataset into different clusters, testing out different cluster sizes\n",
    "\n",
    "clusters = [2, 3, 4, 5, 10, 15]\n",
    "\n",
    "for i in clusters :\n",
    "    kmeans = KMeans(n_clusters = i, n_init = 5, n_jobs = -1, random_state = 42)\n",
    "    kmeans.fit(X3)\n",
    "    print(\"Number of clusters:\", i)\n",
    "    common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "    for num, centroid in enumerate(common_words):\n",
    "        print(str(num) + ' : ' + ', '.join(word_features3[word] for word in centroid))\n",
    "    new_col_name = str(i) + \"_clusters\"\n",
    "    tweets[new_col_name] = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(tweets.shape[0]) :\n",
    "#     if (tweets.iloc[i][\"5_clusters\"] == 1) :\n",
    "#         print(\"Cluster:\", tweets.iloc[i][\"5_clusters\"])\n",
    "#         print(tweets.iloc[i][\"text\"])\n",
    "#         print(\"------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking through the different clusters and its separations, we have determined that the tweet categories can be separated fairly well with only 5 clusters. Therefore, only the results of the 5 clusters are kept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the 5_cluster separation with tweet data\n",
    "nlp_clusters = pd.DataFrame(tweets[\"5_clusters\"])\n",
    "nlp_clusters = nlp_clusters.rename(columns = {\"5_clusters\" : \"nlp_clusters\"})\n",
    "df_connections = df_connections.join(nlp_clusters, on = \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_connections = df_connections.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label clusters with their relevant categories\n",
    "df_connections[\"nlp_clusters\"] = df_connections[\"nlp_clusters\"].astype(str)\n",
    "df_connections[\"nlp_clusters\"] = df_connections[\"nlp_clusters\"].replace({\"0\" : \"Political/Conspiracy\",\n",
    "                                                                         \"1\" : \"Other\",\n",
    "                                                                         \"2\" : \"Info-Sharing\",\n",
    "                                                                         \"3\" : \"COVID Vaccine\",\n",
    "                                                                         \"4\" : \"Other\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Edges and Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets by Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get information required for creating an edges list\n",
    "edges_user = df_connections[[\"author_username\", \"source_username\", \"text\"]]\n",
    "\n",
    "#set columns names as specified by Polinode\n",
    "edges_user.columns = [\"Source\", \"Target\", \"weight\"]\n",
    "\n",
    "#drop columns with NAs\n",
    "edges_user = edges_user.dropna()\n",
    "\n",
    "#create an unweighted version of edges for later calculations purposes\n",
    "edges_user_unweighted = edges_user.copy()\n",
    "\n",
    "#create a weighted version for edges\n",
    "edges_user = edges_user.groupby([\"Source\", \"Target\"]).count().reset_index()\n",
    "edges_user = edges_user.sort_values(\"weight\", ascending = False).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SputnikInt</td>\n",
       "      <td>SputnikInt</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>keryn_1</td>\n",
       "      <td>carnivalist2</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>carnivalist2</td>\n",
       "      <td>carnivalist2</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>carnivalist2</td>\n",
       "      <td>keryn_1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TheHiddenJewell</td>\n",
       "      <td>TheHiddenJewell</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Source           Target  weight\n",
       "0       SputnikInt       SputnikInt      41\n",
       "1          keryn_1     carnivalist2      28\n",
       "2     carnivalist2     carnivalist2      27\n",
       "3     carnivalist2          keryn_1      17\n",
       "4  TheHiddenJewell  TheHiddenJewell      16"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_user.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get information and attributes needed for nodes\n",
    "nodes_user = df_users[['username', 'verified', 'location', 'created_at', 'description',\n",
    "                       'public_metrics.followers_count', 'public_metrics.following_count',\n",
    "                       'public_metrics.tweet_count']]\n",
    "\n",
    "#set columns names as specified by Polinode\n",
    "nodes_user =  nodes_user.rename(columns = {\"username\" : \"Name\", \"public_metrics.followers_count\" : \"followers_count\",\n",
    "                                           \"public_metrics.following_count\" : \"following_count\",\n",
    "                                           \"public_metrics.tweet_count\" : \"tweet_count\"})\n",
    "\n",
    "#ensure the node list only have users appearing on the edges list\n",
    "nodes_user = nodes_user[nodes_user[\"Name\"].isin(edges_user[\"Source\"].append(edges_user[\"Target\"]).dropna().tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>verified</th>\n",
       "      <th>location</th>\n",
       "      <th>created_at</th>\n",
       "      <th>description</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>following_count</th>\n",
       "      <th>tweet_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thereal_truther</td>\n",
       "      <td>False</td>\n",
       "      <td>California</td>\n",
       "      <td>2015-11-29T21:35:35.000Z</td>\n",
       "      <td>Exposing the lies, propaganda &amp; hate of the an...</td>\n",
       "      <td>7014</td>\n",
       "      <td>5484</td>\n",
       "      <td>120569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FloBo2018</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-28T07:03:02.000Z</td>\n",
       "      <td>Parler: @FloBo\\nAlso @therealFloBo1\\n\\n#1A #2A...</td>\n",
       "      <td>3026</td>\n",
       "      <td>4555</td>\n",
       "      <td>26805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and_kell</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2009-05-06T18:25:00.000Z</td>\n",
       "      <td>Find me on Parler @andkell</td>\n",
       "      <td>4477</td>\n",
       "      <td>1812</td>\n",
       "      <td>82057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PupBellus</td>\n",
       "      <td>False</td>\n",
       "      <td>Manchester, England</td>\n",
       "      <td>2011-10-11T02:42:16.000Z</td>\n",
       "      <td>Rubber Pup, Dom, Ferral, Playful, Happy, Naughty!</td>\n",
       "      <td>2321</td>\n",
       "      <td>350</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MrCrazee</td>\n",
       "      <td>False</td>\n",
       "      <td>Plymouth, Devon</td>\n",
       "      <td>2009-01-26T22:16:00.000Z</td>\n",
       "      <td>Independent Parliamentary Candidate for Plymou...</td>\n",
       "      <td>2269</td>\n",
       "      <td>2653</td>\n",
       "      <td>11063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Name verified             location                created_at  \\\n",
       "0  thereal_truther    False           California  2015-11-29T21:35:35.000Z   \n",
       "1        FloBo2018    False                  NaN  2018-01-28T07:03:02.000Z   \n",
       "2         and_kell    False                  NaN  2009-05-06T18:25:00.000Z   \n",
       "3        PupBellus    False  Manchester, England  2011-10-11T02:42:16.000Z   \n",
       "4         MrCrazee    False      Plymouth, Devon  2009-01-26T22:16:00.000Z   \n",
       "\n",
       "                                         description followers_count  \\\n",
       "0  Exposing the lies, propaganda & hate of the an...            7014   \n",
       "1  Parler: @FloBo\\nAlso @therealFloBo1\\n\\n#1A #2A...            3026   \n",
       "2                         Find me on Parler @andkell            4477   \n",
       "3  Rubber Pup, Dom, Ferral, Playful, Happy, Naughty!            2321   \n",
       "4  Independent Parliamentary Candidate for Plymou...            2269   \n",
       "\n",
       "  following_count tweet_count  \n",
       "0            5484      120569  \n",
       "1            4555       26805  \n",
       "2            1812       82057  \n",
       "3             350         848  \n",
       "4            2653       11063  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add columns to tweets dataframe to separate out the activities of the source tweets\n",
    "edges_tweet = df_connections[[\"id\", \"replied_to_tweet_id\", \"quoted_tweet_id\", \"retweeted_id\"]]\n",
    "source = []\n",
    "target = []\n",
    "\n",
    "for i in edges_tweet.index :\n",
    "    if (edges_tweet[\"replied_to_tweet_id\"].loc[i] == edges_tweet[\"replied_to_tweet_id\"].loc[i]) :\n",
    "        source.append(edges_tweet[\"id\"].loc[i])\n",
    "        target.append(edges_tweet[\"replied_to_tweet_id\"].loc[i])\n",
    "    if (edges_tweet[\"quoted_tweet_id\"].loc[i] == edges_tweet[\"quoted_tweet_id\"].loc[i]) :\n",
    "        source.append(edges_tweet[\"id\"].loc[i])\n",
    "        target.append(edges_tweet[\"quoted_tweet_id\"].loc[i])\n",
    "    if (edges_tweet[\"retweeted_id\"].loc[i] == edges_tweet[\"retweeted_id\"].loc[i]) :\n",
    "        source.append(edges_tweet[\"id\"].loc[i])\n",
    "        target.append(edges_tweet[\"retweeted_id\"].loc[i])\n",
    "        \n",
    "edges_tweet = pd.DataFrame(columns = [\"Source\", \"Target\"])\n",
    "edges_tweet[\"Source\"] = source\n",
    "edges_tweet[\"Target\"] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only tweets that were not deleted by Twitter\n",
    "edges_tweet = edges_tweet[edges_tweet[\"Target\"].isin(df_connections[\"id\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get attributes needed for nodes\n",
    "nodes_tweet = df_connections[[\"id\", \"text\", \"hashtag_novax\", \"hashtag_antivax\", \"hashtag_cdcwhistleblower\",\n",
    "                              \"hashtag_vaccineinjury\", \"hashtag_vaxxed\", \"hashtag_cdcfraud\", \"stance\",\n",
    "                             \"author_username\", \"source_username\", \"nlp_clusters\"]]\n",
    "\n",
    "# drop duplicated tweet ids\n",
    "nodes_tweet = nodes_tweet.drop_duplicates(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of tweets included in edges and drop duplicates\n",
    "tweet_ids = edges_tweet[\"Source\"].append(edges_tweet[\"Target\"]).drop_duplicates()\n",
    "tweet_ids = tweet_ids.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ensure the nodes only have tweets that are included in edges\n",
    "nodes_tweet = nodes_tweet[nodes_tweet[\"id\"].isin(tweet_ids)]\n",
    "nodes_tweet = nodes_tweet.rename(columns = {\"id\" : \"Name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export all edges and nodes files\n",
    "edges_user.to_csv(\"output/Combined Tweets/2020-12-24 edges_user.csv\", index = False)\n",
    "edges_user_unweighted.to_csv(\"output/Combined Tweets/2020-12-24 edges_user_unweighted.csv\", index = False)\n",
    "edges_tweet.to_csv(\"output/Combined Tweets/2020-12-24 edges_tweet.csv\", index = False)\n",
    "nodes_user.to_csv(\"output/Combined Tweets/2020-12-24 nodes_user.csv\", index = False)\n",
    "nodes_tweet.to_csv(\"output/Combined Tweets/2020-12-24 nodes_tweet.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
